# /// script
# dependencies = [
#     "langgraph",
#     "langchain-openai",
#     "langchain-core",
#     "python-dotenv"
# ]
# ///

import operator
import uuid
from typing import TypedDict, Annotated, List
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from pydantic import BaseModel, Field

from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

from utils.visualizer import visualize_graph

load_dotenv()

# ==========================================
# 1. State Definitions
# ==========================================

# å…¨å±€çŠ¶æ€
class OverallState(TypedDict):
    topic: str
    subjects: List[str]
    jokes: Annotated[List[str], operator.add]
    final_report: str
    # [æ–°å¢ž]: ä¸ºäº†æ”¯æŒ Audit Logï¼Œæˆ‘ä»¬åŠ å…¥ messages åˆ—è¡¨
    # ä½¿ç”¨ add_messages reducerï¼ŒLangGraph ä¼šè‡ªåŠ¨å¤„ç†å¹¶å‘å†™å…¥æ—¶çš„åˆå¹¶
    messages: Annotated[List[BaseMessage], add_messages]

# Graph åœ¨å»ºç«‹ä¹‹åˆå¹¶ä¸çŸ¥é“ WorkerStateï¼Œè¿™åªæ˜¯ä¸€ä¸ªå±€éƒ¨ä½¿ç”¨çš„çŠ¶æ€
class WorkerState(TypedDict):
    section_subject: str

# ==========================================
# 2. Models & Nodes
# ==========================================

# è¿™é‡Œçš„ llms éƒ½åªçœ‹åˆ°å½“å‰çš„è¾“å…¥ï¼Œä¸ä¼šçœ‹åˆ°åŽ†å²è®°å½•ã€‚
llm_planner = ChatOpenAI(model="gpt-4o-mini", temperature=0.6)

class Subjects(BaseModel):
    subjects: List[str] = Field(description="A list of subjects to generate jokes about")

def planner_node(state: OverallState):
    print(f"--- [Planner] Analyzing topic: {state['topic']} ---")
    
    structured_llm = llm_planner.with_structured_output(Subjects)
    structured_result = structured_llm.invoke(f"Extract subjects from: {state['topic']}")
    
    # [Action]: è¿”å›ž subjects æ•°æ®ï¼ŒåŒæ—¶è®°å½•ä¸€æ¡ AIMessage åˆ°åŽ†å²è®°å½•
    return {
        "subjects": structured_result.subjects,
        "messages": [AIMessage(content=f"PLANNER: I have split the task into: {structured_result.subjects}")]
    }

llm_joke = ChatOpenAI(model="gpt-4.1-nano", temperature=0.9)

def generation_node(state: WorkerState):
    subject = state["section_subject"]
    print(f"  -> [Worker] Processing: {subject}")
    
    response = llm_joke.invoke(f"Tell me a one-sentence joke about {subject}.")
    joke = f"{subject.upper()}: {response.content}"
    
    # [Action]: è¿”å›ž joke æ•°æ®ï¼ŒåŒæ—¶è®°å½•ä¸€æ¡ AIMessage
    # æ³¨æ„ï¼šåœ¨å¹¶å‘æ‰§è¡Œæ—¶ï¼Œå¤šä¸ª Worker ä¼šåŒæ—¶å¾€ messages é‡Œ append
    return {
        "jokes": [joke],
        "messages": [AIMessage(content=f"WORKER({subject}): Generated joke -> {response.content}")]
    }

def reducer_node(state: OverallState):
    jokes = state.get("jokes", [])
    subjects = state.get("subjects", [])
    
    # [Check]: Wait for all subjects to be processed
    if len(jokes) < len(subjects):
        return {}
    
    # [Check]: Avoid duplicate execution (Idempotency)
    if state.get("final_report"):
        return {}

    print("--- [Reducer] Combining results ---")
    summary = "\n".join(jokes)
    
    final_msg = f"Here is the collected humor report:\n\n{summary}"
    
    return {
        "final_report": final_msg,
        "messages": [AIMessage(content="REDUCER: All tasks finished. Report compiled.")]
    }

# ==========================================
# 3. Logic & Helper
# ==========================================
def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"section_subject": s}) for s in state["subjects"]]

# --- ä½ çš„å®¡è®¡å‡½æ•° ---
def print_audit_log(graph, config):
    """æ‰“å°å®Œæ•´çš„å¯¹è¯åŽ†å²å®¡è®¡æ—¥å¿—"""
    print("\n" + "="*60)
    print("ðŸ“œ  FULL CONVERSATION HISTORY (AUDIT LOG)")
    print("="*60)

    # ä»Ž MemorySaver ä¸­è¯»å–æœ€ç»ˆçŠ¶æ€
    final_snapshot = graph.get_state(config)
    all_messages = final_snapshot.values.get("messages", [])

    for i, msg in enumerate(all_messages):
        role = "UNKNOWN"
        icon = "â“"
        if isinstance(msg, HumanMessage):
            role, icon = "HUMAN", "ðŸ‘¤"
        elif isinstance(msg, AIMessage):
            # è¿™é‡Œæˆ‘ä»¬é€šè¿‡ content å‰ç¼€æ¥åŒºåˆ†æ˜¯ Planner è¿˜æ˜¯ Worker
            role, icon = "AI   ", "ðŸ¤–"
        elif isinstance(msg, ToolMessage):
            role, icon = "TOOL ", "ðŸ› ï¸"

        content = msg.content
        print(f"{icon}  [{role}]: {content}")
        print("-" * 60)

# ==========================================
# 4. Graph Construction
# ==========================================
def main():
    # [Setup]: ä½¿ç”¨ MemorySaver
    checkpointer = MemorySaver()
    
    # è¿™é‡Œå®šä¹‰äº† OverallState ä½œä¸º StateGraph çš„ State ç±»åž‹ï¼Œç±»ä¼¼äºŽä¸€ä¸ªå…¨å±€çŠ¶æ€ã€‚
    builder = StateGraph(OverallState)
    builder.add_node("planner", planner_node)
    builder.add_node("generate_joke", generation_node)
    builder.add_node("reduce", reducer_node)

    builder.add_edge(START, "planner")
    builder.add_conditional_edges("planner", continue_to_jokes, ["generate_joke"])
    # [Changed]: Directly connect to reducer, let reducer handle synchronization
    builder.add_edge("generate_joke", "reduce")
    builder.add_edge("reduce", END)

    # å…¨å›¾å…±äº«å†…å­˜ checkpointerï¼Œç”¨äºŽåœ¨å¤šä¸ªèŠ‚ç‚¹ä¹‹é—´å…±äº«çŠ¶æ€ã€‚
    graph = builder.compile(checkpointer=checkpointer)
    
    visualize_graph(graph, "06_map_reduce.png")

    # ==========================================
    # 5. Execution
    # ==========================================
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    user_input = "Tell me jokes about basketball, dogs, and python."
    
    print(f"User Request: {user_input}\n")

    # åˆå§‹è¾“å…¥ä¹Ÿè¦åŒ…å« messagesï¼Œä»¥ä¾¿ Audit Log èƒ½æ˜¾ç¤º User Request
    initial_state = {
        "topic": user_input,
        "messages": [HumanMessage(content=user_input)]
    }

    # è¿è¡Œæµ
    # æ³¨æ„ï¼šMap-Reduce å¯èƒ½ä¼šäº§ç”Ÿå¾ˆå¤š stepsï¼Œstream_mode="updates" å¯ä»¥çœ‹åˆ°æ¯ä¸€æ­¥è°å®Œæˆäº†
    for event in graph.stream(initial_state, config=config):
        # è¿™é‡Œæˆ‘ä»¬åªæ‰“å°ç®€å•çš„è¿›åº¦ç‚¹ï¼Œè¯¦ç»†çš„çœ‹ Audit Log
        for key, value in event.items():
            if key == "generate_joke":
                # å°è¯•ä»Ž message é‡Œæå– infoï¼Œæˆ–è€…ç›´æŽ¥æ‰“å°
                print(f" âœ… Worker finished a task.")

    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    final_state = graph.get_state(config).values
    if "final_report" in final_state:
        print("\n" + "="*40)
        print(f"FINAL REPORT:\n{final_state['final_report']}")
    else:
        print("\n" + "="*40)
        print("FINAL REPORT: Not generated.")

    # æ‰“å°å®¡è®¡æ—¥å¿—
    print_audit_log(graph, config)

if __name__ == "__main__":
    main()