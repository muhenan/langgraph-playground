# /// script
# dependencies = [
#     "langgraph",
#     "langchain-openai",
#     "langchain-core",
#     "python-dotenv"
# ]
# ///


"""
Plan-and-Execute (è§„åˆ’-æ‰§è¡Œ) æ¶æ„
æ ¸å¿ƒéšå–»ï¼šå»ºç­‘å·¥ç¨‹é˜Ÿ

Planner (è®¾è®¡å¸ˆ): ä¸å¹²è„æ´»ç´¯æ´»ã€‚åªè´Ÿè´£çœ‹å®¢æˆ·éœ€æ±‚ï¼Œç”»å‡ºå›¾çº¸ï¼ˆStep 1, 2, 3ï¼‰ã€‚
Executor (æ–½å·¥å‘˜): ä¸çœ‹æ•´å¼ å›¾çº¸ï¼Œåªçœ‹â€œä»Šå¤©å¹²ä»€ä¹ˆâ€ã€‚ä½†ä»–éœ€è¦çŸ¥é“æ˜¨å¤©å¹²äº†ä»€ä¹ˆï¼ˆContextï¼‰ã€‚
Re-Planner (å·¥å¤´/ç›‘ç†): æ¯å¤©å¹²å®Œæ´»æ¥æ£€æŸ¥ã€‚æ ¹æ®è¿›åº¦è°ƒæ•´è®¡åˆ’ï¼Œæˆ–è€…ç­¾å­—éªŒæ”¶ã€‚

å…³é”®ç»„ä»¶è§£æï¼š
1. Planner (The Brain): å°†æ¨¡ç³Šç›®æ ‡è½¬åŒ–ä¸ºç»“æ„åŒ–ä»»åŠ¡åˆ—è¡¨ã€‚å¼ºè¿«æ¨¡å‹åœ¨åŠ¨æ‰‹å‰å…ˆ CoTã€‚
2. Executor (The Hands): ä¸“æ³¨æ‰§è¡Œå½“å‰ä»»åŠ¡ã€‚State Continuity (çŠ¶æ€è¿ç»­æ€§) è‡³å…³é‡è¦ï¼Œå¿…é¡»çœ‹åˆ°å†å²è®°å¿†ã€‚
3. Re-Planner (The Reflector): é—­ç¯åé¦ˆç³»ç»Ÿã€‚è®©ç³»ç»Ÿæœ‰äº†â€œçº é”™â€å’Œâ€œé€‚åº”â€çš„èƒ½åŠ›ã€‚

ä¼˜ç¼ºç‚¹åˆ†æ (Trade-offs):
âœ… Pros: è§£å†³é•¿éš¾ä»»åŠ¡ï¼ˆå¦‚å†™ä»£ç ï¼‰ã€é²æ£’æ€§ï¼ˆå…è®¸ä¸­é€”å‡ºé”™ï¼‰ã€å¯è§‚æµ‹æ€§ã€‚
âŒ Cons: æ…¢ & è´µï¼ˆæ­¥éª¤å¤šï¼ŒLLMè°ƒç”¨æ¬¡æ•°å¤šï¼‰ã€ä¸Šä¸‹æ–‡å †ç§¯ï¼ˆPrompt è¶Šæ¥è¶Šé•¿ï¼‰ã€‚
"""

import operator
from typing import Annotated, List, Tuple, TypedDict, Optional
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

from langgraph.graph import StateGraph, START, END
from utils.visualizer import visualize_graph

load_dotenv()

# ==========================================
# 1. State Definitions
# ==========================================

class PlanExecuteState(TypedDict):
    input: str                          # åŸå§‹å¤§ç›®æ ‡
    plan: List[str]                     # å¾…æ‰§è¡Œçš„ä»»åŠ¡æ ˆ
    # æ ¸å¿ƒï¼šè®°å½•å†å²æ­¥éª¤ [(Step Name, Result Content), ...]
    past_steps: Annotated[List[Tuple[str, str]], operator.add] 
    response: Optional[str]             # æœ€ç»ˆç­”æ¡ˆ

# ==========================================
# 2. Schema (Structured Output)
# ==========================================

class Plan(BaseModel):
    """Planner çš„äº§å‡º"""
    steps: List[str] = Field(description="List of steps to follow, in order.")

class Response(BaseModel):
    """Re-Planner çš„äº§å‡º"""
    response: Optional[str] = Field(description="Final answer to the user, if done.")
    plan: Optional[List[str]] = Field(description="Updated plan (remaining steps), if not done.")

# ==========================================
# 3. Nodes (Pure Logic)
# ==========================================

# ç”¨æ›´å°çš„æ¨¡å‹æœ‰å¹»è§‰ï¼Œä¼šä¸¢ä¸Šä¸‹æ–‡ï¼Œä¸¢æ­¥éª¤
llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0)
# llm = ChatOpenAI(model="gpt-4o", temperature=0)

def planner_node(state: PlanExecuteState):
    """
    Node 1: Planner (å¤§è„‘)
    ä½œç”¨: å°†æ¨¡ç³Šçš„è‡ªç„¶è¯­è¨€ç›®æ ‡ï¼ˆinputï¼‰è½¬åŒ–ä¸ºç»“æ„åŒ–çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆplanï¼‰ã€‚
    ä¸ºä»€ä¹ˆé‡è¦: å¤„ç†é•¿é“¾è·¯ä»»åŠ¡æ—¶ï¼Œå¼ºè¿«æ¨¡å‹å…ˆç†é¡ºé€»è¾‘ã€‚
    """
    print(f"--- [Planner] Strategizing for: {state['input']} ---")
    
    planner_llm = llm.with_structured_output(Plan)
    prompt = (
        "For the given objective, come up with a simple step-by-step plan. "
        "The result of the final step should be the final answer."
    )
    
    plan = planner_llm.invoke([
        SystemMessage(content=prompt),
        HumanMessage(content=state["input"])
    ])
    
    print(f"ğŸ“‹ Initial Plan: {plan.steps}")
    # åˆå§‹åŒ– Plan
    return {"plan": plan.steps}

def executor_node(state: PlanExecuteState):
    """
    Node 2: Executor (æ‰§è¡Œè€…)
    ä½œç”¨: ä»ä»»åŠ¡æ ˆé¡¶å–å‡ºä¸€ä¸ª task æ‰§è¡Œã€‚
    State Continuity: å¿…é¡»æŠŠâ€œå†å²è®°å¿†â€ (past_steps) ä¼ ç»™å½“å‰æ“ä½œè€…ï¼Œå¦åˆ™å®ƒæ˜¯â€œçå­â€ã€‚
    """
    plan = state["plan"]
    task = plan[0]
    
    print(f"--- [Executor] Working on: '{task}' ---")
    
    # [å…³é”®ä¿®å¤]: æ„å»ºä¸Šä¸‹æ–‡
    # æŠŠä¹‹å‰åšè¿‡çš„æ­¥éª¤å’Œç»“æœæ‹¼èµ·æ¥
    context = ""
    if state["past_steps"]:
        context = "Here is the context of what has been done so far:\n"
        for step, result in state["past_steps"]:
            context += f"Step: {step}\nResult: {result}\n---\n"
    
    # å°†ä¸Šä¸‹æ–‡ + å½“å‰ä»»åŠ¡ä¸€èµ·å‘ç»™ LLM
    executor_prompt = (
        "You are a helpful worker. "
        "Execute the following task to the best of your ability."
        "Use the provided context if necessary to complete the task."
        "Provide a concise result."
    )
    
    result = llm.invoke([
        SystemMessage(content=executor_prompt),
        HumanMessage(content=f"{context}\n\nCurrent Task: {task}")
    ])
    
    output = result.content
    print(f"âœ… Result: {output}")
    
    return {
        "past_steps": [(task, output)]
    }

def replanner_node(state: PlanExecuteState):
    """
    Node 3: Re-Planner (åæ€è€…)
    ä½œç”¨: åŠ¨æ€è°ƒæ•´è®¡åˆ’ã€‚
    è¾“å…¥: Goal + Reality (å·²å®Œæˆçš„äº‹å®)
    è¾“å‡º: Gap (å‰©ä¸‹çš„è®¡åˆ’) æˆ– Response (æœ€ç»ˆç­”æ¡ˆ)
    è¿™æ˜¯æ¶æ„ä¸­æœ€æ€§æ„Ÿçš„éƒ¨åˆ†ï¼Œæä¾›äº†â€œçº é”™â€èƒ½åŠ›ã€‚
    """
    print("--- [Re-Planner] Updating Plan... ---")
    
    replanner_llm = llm.with_structured_output(Response)
    
    # æ„é€ ä¸Šä¸‹æ–‡ï¼šç›®æ ‡ + åŸè®¡åˆ’ + å·²å®Œæˆ
    # è¿™é‡Œçš„ prompt å†³å®šäº† Agent æœ‰å¤š"èªæ˜"
    past_steps_format = "\n".join([f"Step: {s}\nResult: {r}" for s, r in state["past_steps"]])
    
    # æ ¸å¿ƒä¿®æ”¹ï¼šåŠ å¼ºäº† Instructions éƒ¨åˆ†çš„é€»è¾‘çº¦æŸ
    prompt = f"""
    Your objective: {state['input']}
    
    Original Plan: {state['plan']}
    
    Completed Steps:
    {past_steps_format}
    
    Instructions:
    1. Analyze the "Completed Steps". Did the last step successfully produce the final answer for the "Objective"?
    2. IF YES (Objective is Done):
       - You MUST output the final answer in the 'response' field.
       - The 'response' should be a synthesis of the execution results.
       - Set 'plan' to [].
       - **CRITICAL**: You cannot return an empty plan without a response. If the plan is empty, 'response' MUST contain the answer.
       
    3. IF NO (Objective is NOT Done):
       - Return a new list of *remaining* steps in 'plan'.
       - Remove the step that was just completed.
       - Do NOT set 'response'.
    """
    
    result = replanner_llm.invoke(prompt)
    
    # è¿™é‡Œçš„ Python é€»è¾‘ä¿æŒç®€å•å³å¯ï¼Œå› ä¸ºæˆ‘ä»¬ç›¸ä¿¡ LLM ä¼šéµå¾ªä¸Šé¢çš„ CRITICAL æŒ‡ä»¤
    if result.response:
        print("ğŸ‰ [Re-Planner] Finished via Response!")
        print(f"ğŸ‰ [Re-Planner] Response: {result.response}")
        return {"response": result.response, "plan": []}
    else:
        print(f"ğŸ”„ [Re-Planner] New Plan: {result.plan}")
        return {"plan": result.plan}

# ==========================================
# 4. Graph Logic
# ==========================================

def router(state: PlanExecuteState):
    # 1. å¦‚æœæœ‰æœ€ç»ˆå›å¤ï¼Œç»“æŸ
    if state.get("response"):
        return END
    
    # 2. [ä¿®å¤] å¦‚æœè®¡åˆ’è¡¨ç©ºäº†ï¼Œä¹Ÿæ²¡æ´»å¹²äº†ï¼Œå¼ºåˆ¶ç»“æŸ
    if not state.get("plan"): # ç©ºåˆ—è¡¨åœ¨ Python ä¸­ä¸º False
        return END
        
    # 3. è¿˜æœ‰æ´»ï¼Œç»§ç»­å¹²
    return "executor"

def main():
    builder = StateGraph(PlanExecuteState)
    
    builder.add_node("planner", planner_node)
    builder.add_node("executor", executor_node)
    builder.add_node("re_planner", replanner_node)
    
    # çº¿æ€§æµï¼šStart -> Plan -> Exec -> RePlan
    builder.add_edge(START, "planner")
    builder.add_edge("planner", "executor")
    builder.add_edge("executor", "re_planner")
    
    # å¾ªç¯ç‚¹ï¼šRePlan -> Exec (æˆ–è€… END)
    builder.add_conditional_edges(
        "re_planner",     # Source
        router,           # Function (å†³å®šå»å“ª)
        ["executor", END] # Path Map (å‘Šè¯‰ç”»å›¾å·¥å…·ï¼šåªæœ‰è¿™ä¸¤æ¡è·¯)
    )
    
    graph = builder.compile()
    visualize_graph(graph, "10_plan_exec_pure.png")
    
    # Run
    user_query = "Write a haiku about recursion, then explain it, then translate the explanation to French."
    print(f"User Query: {user_query}\n")
    
    config = {"recursion_limit": 20}
    
    # åªéœ€è¦ç®€å•çš„ stream å³å¯
    for event in graph.stream({"input": user_query}, config=config):
        pass # æ—¥å¿—å·²ç»åœ¨èŠ‚ç‚¹å†…éƒ¨æ‰“å°äº†

    # è·å–æœ€ç»ˆç»“æœ
    # æ³¨æ„ï¼šåœ¨ stream ç»“æŸåï¼Œæˆ‘ä»¬é€šå¸¸æ— æ³•ç›´æ¥è·å¾—æœ€åçš„çŠ¶æ€å¯¹è±¡ï¼Œé™¤éä½¿ç”¨ checkpointer
    # ä½†æˆ‘ä»¬å¯ä»¥æ‰“å°æœ€åä¸€æ¬¡ event æˆ–è€…ä¸Šé¢çš„æ—¥å¿—æ¥éªŒè¯

if __name__ == "__main__":
    main()